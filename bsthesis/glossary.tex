%!TEX root = thesis.tex
%Term definitions
\newacronym{ANN}{ANN}{artificial neural network}
\newacronym{CSR}{CSR}{cursive script recognition}
\newacronym{DTW}{DTW}{dynamic time warping}
\newacronym{HMM}{HMM}{hidden Markov model}
\newacronym{HWR}{HWR}{handwriting recognition}
\newacronym{MLP}{MLP}{multilayer perceptron}
\newacronym{MSE}{MSE}{mean squared error}
\newacronym{OOV}{OOV}{out of vocabulary}
\newacronym{TDNN}{TDNN}{time delay neural network}

% Term definitions
\newglossaryentry{epoch}{name={epoch}, description={During iterative training of a neural network, an \textit{epoch} is a single pass through the entire training set, followed by testing of the verification set.\cite{Concise12}}}

\newglossaryentry{hyperparamter}{name={hyperparamter}, description={A
\textit{hyperparamter} is a parameter of a neural net, that cannot be learned,
but has to be chosen.}, symbol={\ensuremath{\theta}}}

\newglossaryentry{learning rate}{name={learning rate}, description={A factor $0 \leq \eta \in \mdr$ that affects how fast new weights are learned. $\eta=0$ means that no new data is learned}, symbol={\ensuremath{\eta}}} % Andrew Ng: \alpha

\newglossaryentry{learning rate decay}{name={learning rate decay}, description={The learning rate decay $0 < \alpha \leq 1$ is used to adjust the learning rate. After each epoch the learning rate $\eta$ is updated to $\eta \gets \eta \times \alpha$}, symbol={\ensuremath{\eta}}}

\newglossaryentry{preactivation}{name={preactivation}, description={The preactivation of a neuron is the weighted sum of its input, before the activation function is applied.}}

\newglossaryentry{stroke}{name={stroke}, description={The path the pen took from
the point where the pen was put down to the point where the pen was lifted first.}}

\newglossaryentry{weight}{name={weight}, description={A
\textit{weight} is a parameter of a neural net, that can be learned.}, symbol={\ensuremath{\weight}}}