%!TEX root = thesis.tex
\section{Multilayer Perceptron}\label{ch:Content2:sec:Section2}

\Glspl{MLP} are neural nets whichs neurons are structured in layers.
Each layer is fully connected with the next layer, but there are no other
connections between neurons.

\begin{figure}[ht]
    \centering
    \input{figures/feedforward.tex}
    \caption{Feedforward artificial neural network}
    \label{fig:feedforward}
\end{figure}

The red neurons in \cref{fig:feedforward} are input neurons, the green ones are
hidden neurons and the blue one is an output node. The gray neurons are bias neurons.
Bias neurons have a fixed output of $1$.

Usually, you have as many output neurons as you have classes. So in the case of
symbol recognition that would be about $\si{1076}$ neurons.

The number of input neurons is equal to the number of features.

\section{Notation}
Two neighboring layers of neurons are fully connected and have weights between
two layers. This means you can store those weights in form of matrices.
Assuming you have $n$ neurons in layer $i$ and $m$ neurons in layer $i + 1$,
you would have a matrix

\[W_i = \begin{pmatrix}
    w_{1,1} & w_{1,2} & \dots & w_{1,m}\\
    w_{2,1} & w_{2,2} & \dots & w_{2,m}\\
    w_{3,1} & w_{3,2} & \dots & w_{3,m}\\
    \vdots  &         & \ddots& \vdots \\
    w_{n,1} & w_{n,2} & \dots & w_{n,m}
\end{pmatrix}\]

Let $w_{i,j,k}$ be the value $w_{i,j}$ in $W_k$.

So $W_i \in \mdr^{n \times m}$ is the matrix denoting the weights between layer
$i$ and layer $i+1$.

The unweighted output of layer $i$ is denoted by $x_i$; the weighted output
by $\net_i$. Instead of $x_1$ I will write $x$.
The output of the \gls{MLP} for the input $x$ is denoted by $o_x$.

In principle each neuron might have a different activation function, but in
practice each neuron in one layer has the same activation function. However,
activation functions might differ from layer to layer. This is the reason why
I denote the activation function of layer $i$ by $\varphi_i$. Although $\varphi_i$
is defined for single neurons, I will in the following apply it to vectors. In
its meant to be applied pointwise.

Let $n_i$ be the number of neurons in the $i$-th layer and $\layernumber$ be the number of layers of the
\gls{MLP}.

\section{Evaluation}
Let $x_1 \in \mdr^{1 \times n}$ be an unweighted output of layer $1$. So it's
simply the input of our neural net with $n$ features. Let 
$m \in \mdn{N}_{\geq 1}$ be the number of neurons in layer $2$.

Given $x_1$ one can easily compute the weighted input for layer $2$:

\[
\begin{array}{@{}c@{\;}c@{\;}c@{\;}c@{\;}c@{}}
x_1 & \cdot & W_{1} & = & \net_1 \\
\vin && \vin && \vin \\
\mathbb{R}^{1\times n} && \mathbb{R}^{n\times m} && \mathbb{R}^{1\times m}
\end{array}
\]

After that, you can apply the activation function $\varphi_{i+1}$ pointwise
to $\net_i$. %TODO: Is that really called "pointwise" application?

So the output vector $x_3$ of a 3-layer (input, hidden, output) neural net can be
computed by

\[x_3 = \varphi_3(\varphi_2(x_1 \cdot W_{1}) \cdot W_{2})\]

The output of a general \gls{MLP} can be computed by

\begin{align*}
    \Phi(x, 2) &:= \varphi_{2}(x_1 \cdot W_{1})\\
    \Phi(x, n) &:= \varphi_{n} \big (\Phi(x, n-1) \cdot W_{n-1} \big)
\end{align*}

\section{Supervised Training with Backpropagation}\label{sec:training}
The backpropagation algorithm is a supervised algorithm for training
\glspl{MLP}. This means the trainigset $T$ consists of tuples $(x, t_x)$
where $x$ is input and $t_x$ is the desired output.

To evaluate how good the current \gls{MLP} is, an error function can be defined:

\begin{align*}
    E: \mdr^{n_1 \times n_2} \times \mdr^{n_2 \times n_3} \times \dots \times \mdr^{n_{\layernumber-1, \layernumber}} \rightarrow \mdr_{\geq 0}\\
    E_T(W) = \frac{1}{2} \sum_{(x, t_x) \in T} \sum_{p = 1}^{|n_\layernumber|} \left (t_{x}^{(k)} - o_{x}^{(k)} \right )^2
\end{align*}

This function is isomorphic to 

\[E: \mdr^{\sum_{i=2}^\layernumber n_{i-1} \cdot n_{i}} \rightarrow \mdr_{\geq 0}\]

This error should be minimized. As the error is the sum of non-negative values,
we will get a lower error by minimizing the error for a single training example.
However, note that those minimizations are not independant. This means, we could
get trapped in a local minimum.

The idea is to \enquote{go} into the direction in which the error $E$ decreases
most. This is the gradient and the process is thus called \textit{gradient descent}.

At this point we need to decide how far we want to go. If we make too big steps
in the direction of the gradient, we might overshoot. If we make too small steps,
the algorithm will take too long to get to the minimum. As reducing the error
is basically learning the number is called learning rate $\eta \in \mdr_{> 0}$.

So the algorithm is

\begin{algorithm}[h]
    \begin{algorithmic}
        \Function{backpropagate}{$T$, $W$}
            \While{True}
                \ForAll{$(x, t_x) \in T$}
                    \ForAll{node $i$}
                        \ForAll{nodes $j$ following $i$}
                            \State $\displaystyle w_{ijk} \gets w_{ijk} - \eta \frac{\partial E_{\Set{x}}}{\partial w_{ijk}} (W)$
                        \EndFor
                    \EndFor
                \EndFor
            \EndWhile
        \EndFunction
    \end{algorithmic}
\caption{Backpropagate}
\label{alg:backpropagate}
\end{algorithm}

Computing the partial derivatives $\frac{\partial E_{\Set{x}}}{\partial w_{ijk}}$
is not a trivial task. To do that, we have to take a closer look at the
error function:

\begin{align}
    \frac{\partial E_{\Set{x}}}{\partial w_{ijk}}
    &= \frac{\partial}{\partial w_{ijk}} \frac{1}{2} \sum_{(x, t_x) \in T} \sum_{p = 1}^{|n_\layernumber|} \left (t_{x}^{(p)} - o_{x}^{(p)} \right )^2\\
    &= \frac{1}{2} \sum_{(x, t_x) \in T} \sum_{p = 1}^{|n_\layernumber|} \frac{\partial}{\partial w_{ijk}} \left (t_{x}^{(p)} - o_{x}^{(p)} \right )^2\\
    &= \frac{1}{2} \sum_{(x, t_x) \in T} \sum_{p = 1}^{|n_\layernumber|} 2 \left (t_{x}^{(p)} - o_{x}^{(p)} \right ) \cdot \frac{\partial}{\partial w_{ijk}} \left (t_{x}^{(p)} - o_{x}^{(p)} \right )\\
    &= - \sum_{(x, t_x) \in T} \sum_{p = 1}^{|n_\layernumber|} \left (t_{x}^{(p)} - o_{x}^{(p)} \right ) \cdot \frac{\partial}{\partial w_{ijk}} \left (\varphi_{n-1}(\Phi(x, n-1) \cdot W_{n-1,n})^{(p)} \right)
\end{align}

At this point, it doesn't get simpler except if you know where $w_{ijk}$ is.
If $w_{ijk}$ is for example a weight for an output node, you can apply the chain
rule\footnote{$(f(g))' = f'(g) \cdot g'$} once:

\begin{align}
    \frac{\partial E_{\Set{x}}}{\partial w_{ijk}} &= - \sum_{(x, t_x) \in T} \sum_{p = 1}^{|n_\layernumber|} \left (t_{x}^{(p)} - o_{x}^{(p)} \right ) \cdot \left ( \frac{\partial}{\partial w_{ijk}} \varphi_{n-1} \right )(\Phi(x, n-1) \cdot W_{n-1,n})^{(p)}\\
\end{align}