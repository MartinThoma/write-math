%!TEX root = thesis.tex
\section{Multilayer Perceptron}\label{ch:Content2:sec:Section2}

\Glspl{MLP} are neural nets whichs neurons are structured in layers.
Each layer is fully connected with the next layer, but there are no other
connections between neurons.

\begin{figure}[ht]
    \centering
    \input{figures/feedforward.tex}
    \caption{Feedforward artificial neural network}
    \label{fig:feedforward}
\end{figure}

The red neurons in \cref{fig:feedforward} are input neurons, the green ones are
hidden neurons and the blue one is an output node. The gray neurons are bias neurons.
Bias neurons have a fixed output of $1$.

Usually, you have as many output neurons as you have classes. So in the case of
symbol recognition that would be about $\si{1076}$ neurons.

The number of input neurons is equal to the number of features.

Two neighboring layers of neurons are fully connected and have weights between
two layers. This means you can store those weights in form of matrices.
Assuming you have $n$ neurons in layer $i$ and $m$ neurons in layer $i + 1$,
you would have a matrix

\[W_{i} = \begin{pmatrix}
    w_{1,1} & w_{1,2} & \dots & w_{1,m}\\
    w_{2,1} & w_{2,2} & \dots & w_{2,m}\\
    w_{3,1} & w_{3,2} & \dots & w_{3,m}\\
    \vdots  &         & \ddots& \vdots \\
    w_{n,1} & w_{n,2} & \dots & w_{n,m}
\end{pmatrix}\]

This means you can easily get the input for layer $i+1$. As the input neurons
output is $x \in \mdr^{1 \times n}$ you can simply multiply $x \cdot W_1$
to get the output $x \cdot W_1 := \net_j \in \mdr^{1 \times m}$.

After that, you can apply the activation function $\varphi$ to $_i$.
As I assume that the activation function will be the same foreach node in the
same layer $l$, I will simply write $\varphi_l(o)$. This means that $\varphi_l$ is applied
componentwise to each $o_i$ with $i \in 1, \dots, m$.

So the output vector $o$ of a 3-layer (input, hidden, output) neural net can be
computed by

\[o = \varphi_2(\varphi_1(x \cdot W_{1,2}) \cdot W_{2,3})\]

The output of a general \gls{MLP} can be computed by

\begin{align*}
    \Phi(x, 1) &:= \varphi_{1}(x \cdot W_{1,2})\\
    \Phi(x, n) &:= \varphi_{n-1} \big (\Phi(x, n-1) \cdot W_{n-1, n} \big)
\end{align*}

\subsection{Supervised Training with Backpropagation}
The backpropagation algorithm is a supervised algorithm for training an
\gls{MLP}. This means the trainigset $T$ consists of tuples $(x, t)$
where $x$ is input and $t_x$ is the desired output.

The backpropagation algorithm first makes a forward pass to get to know what
the current output $o_x$ would be for each $(x, t_x) \in T$.
To evaluate how good the current \gls{MLP} is, an error function can be defined:

\begin{align*}
    E: \mdr^{n_1 \times n_2} \times \mdr^{n_2 \times n_3} \times \dots \times \mdr^{n_{m-1, m}} \rightarrow \mdr_{\geq 0}\\
    E_T(W) = \frac{1}{2} \sum_{(x, t_x) \in T} \sum_{k \in out} \left (t_{x}^{(k)} - o_{x}^{(k)} \right )^2
\end{align*}

where $n_i$ is the numer of neurons in the $i$-th layer and $m$ is the number
of layers and $out$ is the set of all output neurons.

This function is isomorphic to 

\[E: \mdr^{\sum_{i=2}^m n_{i-1} \cdot n_{i}} \rightarrow \mdr_{\geq 0}\]

This error should be minimized. As the error is the sum of non-negative values,
we will get a lower error by minimizing the error for a single training example.
However, note that those minimizations are not independant. This means, we could
get trapped in a local minimum.

The idea is to \enquote{go} into the direction in which the error $E$ decreases
most. This is the gradient and the process is thus called \textit{gradient descent}.

At this point we need to decide how far we want to go. If we make too big steps
in the direction of the gradient, we might overshoot. If we make too small steps,
the algorithm will take too long to get to the minimum. As reducing the error
is basically learning the number is called learning rate $\eta \in \mdr_{> 0}$.

So the algorithm is

\begin{algorithm}[h]
    \begin{algorithmic}
        \Function{backpropagate}{$T$, $W$}
            \While{True}
                \ForAll{$(x, t_x) \in T$}
                    \ForAll{node $i$}
                        \ForAll{nodes $j$ following $i$}
                            \State $\displaystyle w_{ij} \gets w_{ij} - \eta \frac{\partial E_{\Set{x}}}{\partial w_{ij}} (W)$
                        \EndFor
                    \EndFor
                \EndFor
            \EndWhile
        \EndFunction
    \end{algorithmic}
\caption{Backpropagate}
\label{alg:backpropagate}
\end{algorithm}

Computing the partial derivatives $\frac{\partial E_{\Set{x}}}{\partial w_{ij}}$
is not a trivial task. To do that, we have to take a closer look at the
error function:

Let $n$ be the number of layers of the \gls{MLP}.

\begin{align}
    \frac{\partial E_{\Set{x}}}{\partial w_{ij}}
    &= \frac{\partial}{\partial w_{ij}} \frac{1}{2} \sum_{(x, t_x) \in T} \sum_{k \in out} \left (t_{x}^{(k)} - o_{x}^{(k)} \right )^2\\
    &= \frac{1}{2} \sum_{(x, t_x) \in T} \sum_{k \in out} \frac{\partial}{\partial w_{ij}} \left (t_{x}^{(k)} - o_{x}^{(k)} \right )^2\\
    &= \frac{1}{2} \sum_{(x, t_x) \in T} \sum_{k \in out} 2 \left (t_{x}^{(k)} - o_{x}^{(k)} \right ) \cdot \frac{\partial}{\partial w_{ij}} \left (t_{x}^{(k)} - o_{x}^{(k)} \right )\\
    &= - \sum_{(x, t_x) \in T} \sum_{k \in out} \left (t_{x}^{(k)} - o_{x}^{(k)} \right ) \cdot \frac{\partial}{\partial w_{ij}} \left (\varphi_{n-1}(\Phi(x, n-1) \cdot W_{n-1,n})^{(k)} \right)
\end{align}

At this point, it doesn't get simpler except if you know where $w_{ij}$ is.
If $w_{ij}$ is for example a weight for an output node, you can apply the chain
rule\footnote{$(f(g))' = f'(g) \cdot g'$} once:

\begin{align}
    \frac{\partial E_{\Set{x}}}{\partial w_{ij}} &= - \sum_{(x, t_x) \in T} \sum_{k \in out} \left (t_{x}^{(k)} - o_{x}^{(k)} \right ) \cdot \left ( \frac{\partial}{\partial w_{ij}} \varphi_{n-1} \right )(\Phi(x, n-1) \cdot W_{n-1,n})^{(k)}\\
\end{align}